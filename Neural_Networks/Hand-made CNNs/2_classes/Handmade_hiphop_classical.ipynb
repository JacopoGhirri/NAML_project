{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handmade_hiphop_classical.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Black Box Approach for 2 genres classification - Hip Hop vs Classical\n",
        "\n",
        "We aim at training a Neural Network to distinguish between two genres: Hip Hop and classical."
      ],
      "metadata": {
        "id": "gKB_Sss8Ql-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SwPJ54sBQlFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a229a68c-a695-492a-fa6c-57aa98c67480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "#importing google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the working directory\n",
        "%cd /gdrive/MyDrive/polimi/NAML/NAML_proj/"
      ],
      "metadata": {
        "id": "iYHj7NhnQs_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3baa331b-f313-434b-a44d-2e19cb832200"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/polimi/NAML/NAML_proj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "INc4ttEUQuxN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is composed of Mel-Spectrograms of each audio sample, labeled with respect to each genre"
      ],
      "metadata": {
        "id": "IoV9bIbPfg55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "#genres = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n",
        "genres = {'hiphop': 0, 'classical': 1}\n",
        "n_genres = 2\n",
        "\n",
        "for genre, genre_number in genres.items():\n",
        "    for filename in os.listdir(f'dataset_old/genres/{genre}'):\n",
        "        songname = f'dataset_old/genres/{genre}/{filename}'\n",
        "        y, sr = librosa.load(songname, mono=True, duration=29.7)\n",
        "        ps = librosa.feature.melspectrogram(y=y, sr=sr, hop_length = 256, n_fft = 512)\n",
        "        ps = librosa.power_to_db(ps**2)\n",
        "        dataset.append( (ps, genre_number) )\n",
        "    print(str(genre+' done'))"
      ],
      "metadata": {
        "id": "oiT5mkVqQ_lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bc06ea-1f4e-4f1a-c0f4-6f3fb10a20e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiphop done\n",
            "classical done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset according to the following composition:\n",
        "\n",
        "\n",
        "*   70% training set\n",
        "*   20% validation set\n",
        "*   10% test set\n",
        "\n",
        "Maintaining equal proportions amongst classes\n",
        "\n"
      ],
      "metadata": {
        "id": "0sgOQfv-gZ04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order = np.arange(start = 0, stop = 100, step = 1)\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "training = []\n",
        "validation = []\n",
        "test = []\n",
        "\n",
        "for i in range(n_genres):\n",
        "  shuffle = np.random.permutation(order)\n",
        "  for k in range(70):\n",
        "    training.append(dataset[i*100 + shuffle[k]])\n",
        "  for l in range(20):\n",
        "    validation.append(dataset[i*100 + shuffle[l+70]])\n",
        "  for m in range(10):\n",
        "    test.append(dataset[i*100 + shuffle[m+90]])"
      ],
      "metadata": {
        "id": "OSUK8JPgRFVF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = zip(*training)\n",
        "X_valid, Y_valid = zip(*validation)\n",
        "X_test, Y_test = zip(*test)\n",
        "\n",
        "X_train = np.array([x.reshape( (128, 2559, 1) ) for x in X_train])\n",
        "X_valid = np.array([x.reshape( (128, 2559, 1) ) for x in X_valid])\n",
        "X_test = np.array([x.reshape( (128, 2559, 1) ) for x in X_test])\n",
        "\n",
        "Y_train = np.array(tfk.utils.to_categorical(Y_train, 2))\n",
        "Y_valid = np.array(tfk.utils.to_categorical(Y_valid, 2))\n",
        "Y_test = np.array(tfk.utils.to_categorical(Y_test, 2))"
      ],
      "metadata": {
        "id": "WA29NV-URMuG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proposed model is composed as a stack of convolutional layer, followed by a Global Average Pooling layer leading to a fully connected section.\n",
        "\n",
        "Optimization is performed as a Batch version of Adam optimizer algorithm"
      ],
      "metadata": {
        "id": "O6AvTMw2hgxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, n_units):\n",
        "# Build the neural network layer by layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    conv1 = tfkl.Conv2D(\n",
        "        name = 'conv_1',\n",
        "        filters=4,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(input_layer)\n",
        "    conv1_2 = tfkl.Conv2D(\n",
        "        name = 'conv_1_2',\n",
        "        filters=8,\n",
        "        kernel_size=(2, 2),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(conv1)\n",
        "    pool1 = tfkl.MaxPooling2D(\n",
        "        name = 'pool_1',\n",
        "        pool_size = (2, 2)\n",
        "    )(conv1_2)\n",
        "\n",
        "    conv2 = tfkl.Conv2D(\n",
        "        name = 'conv_2',\n",
        "        filters=16,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(pool1)\n",
        "    pool2 = tfkl.MaxPooling2D(\n",
        "        name = 'pool_2',\n",
        "        pool_size = (2, 2)\n",
        "    )(conv2)\n",
        "\n",
        "    conv3 = tfkl.Conv2D(\n",
        "        name = 'conv_3',\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(pool2)\n",
        "    pool3 = tfkl.MaxPooling2D(\n",
        "        name = 'pool_3',\n",
        "        pool_size = (2, 2)\n",
        "    )(conv3)\n",
        "\n",
        "    conv4 = tfkl.Conv2D(\n",
        "        name = 'conv_4',\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(pool3)\n",
        "    conv4_2 = tfkl.Conv2D(\n",
        "        name = 'conv_4_2',\n",
        "        filters=64,\n",
        "        kernel_size=(2, 2),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(conv4)\n",
        "    pool4 = tfkl.MaxPooling2D(\n",
        "        name = 'pool_4',\n",
        "        pool_size = (2, 2)\n",
        "    )(conv4_2)\n",
        "\n",
        "    conv5 = tfkl.Conv2D(\n",
        "        name = 'conv_5',\n",
        "        filters=128,\n",
        "        kernel_size=(2, 2),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(pool4)\n",
        "    pool5 = tfkl.MaxPooling2D(\n",
        "        name = 'pool_5',\n",
        "        pool_size = (2, 2)\n",
        "    )(conv5)\n",
        "\n",
        "    conv6 = tfkl.Conv2D(\n",
        "        name = 'conv_6',\n",
        "        filters=286,\n",
        "        kernel_size=(1, 1),\n",
        "        strides = (1, 1),\n",
        "        padding = 'same',\n",
        "        activation = 'relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
        "    )(pool5)\n",
        "    \n",
        "\n",
        "    global_average = tfkl.GlobalAveragePooling2D(name = 'GAP')(conv6)\n",
        "    global_average = tfkl.Dropout(0.3,name = 'Dropout_GAP', seed=seed)(global_average)\n",
        "    \n",
        "    classifier_layer = tfkl.Dense(units=64, name='Classifier', activation='relu')(global_average)\n",
        "    \n",
        "    classifier_layer = tfkl.Dropout(0.3, seed=seed, name = 'Dropout_Classifier')(classifier_layer)\n",
        "    classifier_layer_2 = tfkl.Dense(units=32, name='Classifier_2', activation='relu')(classifier_layer)\n",
        "    classifier_layer_2 = tfkl.Dropout(0.25, seed=seed, name = 'Dropout_Classifier_2')(classifier_layer_2)\n",
        "    output_layer = tfkl.Dense(units=n_units, activation='softmax', name='Output')(classifier_layer_2)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
        "\n",
        "    # Return the model\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lR2QkBoSQ8lW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 2559, 1)\n",
        "\n",
        "model = build_model(input_shape, n_genres)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zIitkAlGQ-mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f30785-6b35-4b28-efcb-eae9caa86dcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input (InputLayer)          [(None, 128, 2559, 1)]    0         \n",
            "                                                                 \n",
            " conv_1 (Conv2D)             (None, 128, 2559, 4)      40        \n",
            "                                                                 \n",
            " conv_1_2 (Conv2D)           (None, 128, 2559, 8)      136       \n",
            "                                                                 \n",
            " pool_1 (MaxPooling2D)       (None, 64, 1279, 8)       0         \n",
            "                                                                 \n",
            " conv_2 (Conv2D)             (None, 64, 1279, 16)      1168      \n",
            "                                                                 \n",
            " pool_2 (MaxPooling2D)       (None, 32, 639, 16)       0         \n",
            "                                                                 \n",
            " conv_3 (Conv2D)             (None, 32, 639, 32)       4640      \n",
            "                                                                 \n",
            " pool_3 (MaxPooling2D)       (None, 16, 319, 32)       0         \n",
            "                                                                 \n",
            " conv_4 (Conv2D)             (None, 16, 319, 64)       18496     \n",
            "                                                                 \n",
            " conv_4_2 (Conv2D)           (None, 16, 319, 64)       16448     \n",
            "                                                                 \n",
            " pool_4 (MaxPooling2D)       (None, 8, 159, 64)        0         \n",
            "                                                                 \n",
            " conv_5 (Conv2D)             (None, 8, 159, 128)       32896     \n",
            "                                                                 \n",
            " pool_5 (MaxPooling2D)       (None, 4, 79, 128)        0         \n",
            "                                                                 \n",
            " conv_6 (Conv2D)             (None, 4, 79, 286)        36894     \n",
            "                                                                 \n",
            " GAP (GlobalAveragePooling2D  (None, 286)              0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " Dropout_GAP (Dropout)       (None, 286)               0         \n",
            "                                                                 \n",
            " Classifier (Dense)          (None, 64)                18368     \n",
            "                                                                 \n",
            " Dropout_Classifier (Dropout  (None, 64)               0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " Classifier_2 (Dense)        (None, 32)                2080      \n",
            "                                                                 \n",
            " Dropout_Classifier_2 (Dropo  (None, 32)               0         \n",
            " ut)                                                             \n",
            "                                                                 \n",
            " Output (Dense)              (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,232\n",
            "Trainable params: 131,232\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20, restore_best_weights=True)\n",
        "adaptive_LR = tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-4)\n",
        "\n",
        "standard_history = model.fit(\n",
        "    x = X_train,\n",
        "    y = Y_train,\n",
        "    epochs = 500,\n",
        "    batch_size = 20,\n",
        "    validation_data= (X_valid, Y_valid),\n",
        "    callbacks = [early_stopping, adaptive_LR]\n",
        "    )"
      ],
      "metadata": {
        "id": "yFUsDY6BRWi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ee999e-d66a-4860-865b-82409e833cd6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "7/7 [==============================] - 2s 191ms/step - loss: 0.7323 - accuracy: 0.6000 - val_loss: 0.6001 - val_accuracy: 0.5000 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "7/7 [==============================] - 1s 123ms/step - loss: 0.6147 - accuracy: 0.7071 - val_loss: 0.4249 - val_accuracy: 0.9500 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "7/7 [==============================] - 1s 135ms/step - loss: 0.3965 - accuracy: 0.8643 - val_loss: 0.1697 - val_accuracy: 0.9250 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.3086 - accuracy: 0.8857 - val_loss: 0.0693 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.1794 - accuracy: 0.9357 - val_loss: 0.0318 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.1531 - accuracy: 0.9500 - val_loss: 0.0502 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "7/7 [==============================] - 1s 131ms/step - loss: 0.1348 - accuracy: 0.9500 - val_loss: 0.0397 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "7/7 [==============================] - 1s 132ms/step - loss: 0.1995 - accuracy: 0.9357 - val_loss: 0.0382 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.2055 - accuracy: 0.9571 - val_loss: 0.0186 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.1264 - accuracy: 0.9643 - val_loss: 0.0342 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.0787 - accuracy: 0.9786 - val_loss: 0.0225 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.1259 - accuracy: 0.9500 - val_loss: 0.0060 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "7/7 [==============================] - 1s 123ms/step - loss: 0.1394 - accuracy: 0.9500 - val_loss: 0.0050 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "7/7 [==============================] - 1s 134ms/step - loss: 0.0896 - accuracy: 0.9714 - val_loss: 0.0135 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0565 - accuracy: 0.9857 - val_loss: 0.0067 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0721 - accuracy: 0.9786 - val_loss: 0.0032 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0419 - accuracy: 0.9857 - val_loss: 0.0014 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0567 - accuracy: 0.9857 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.0476 - accuracy: 0.9857 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0460 - accuracy: 0.9786 - val_loss: 0.0022 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "7/7 [==============================] - 1s 132ms/step - loss: 0.0423 - accuracy: 0.9857 - val_loss: 0.0020 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.1186 - accuracy: 0.9571 - val_loss: 0.0031 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0811 - accuracy: 0.9786 - val_loss: 0.0149 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 24/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.0448 - accuracy: 0.9857 - val_loss: 0.0059 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 25/500\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0475 - accuracy: 0.9786 - val_loss: 0.0039 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 26/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.0544 - accuracy: 0.9857 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 27/500\n",
            "7/7 [==============================] - 1s 123ms/step - loss: 0.0465 - accuracy: 0.9857 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
            "Epoch 28/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0299 - accuracy: 0.9857 - val_loss: 0.0027 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 29/500\n",
            "7/7 [==============================] - 1s 121ms/step - loss: 0.0330 - accuracy: 0.9857 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 30/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0262 - accuracy: 0.9929 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 31/500\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0312 - accuracy: 0.9857 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 32/500\n",
            "7/7 [==============================] - 1s 123ms/step - loss: 0.0418 - accuracy: 0.9857 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 33/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0374 - accuracy: 0.9786 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
            "Epoch 34/500\n",
            "7/7 [==============================] - 1s 122ms/step - loss: 0.0245 - accuracy: 0.9857 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
            "Epoch 35/500\n",
            "7/7 [==============================] - 1s 132ms/step - loss: 0.0281 - accuracy: 0.9857 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
            "Epoch 36/500\n",
            "7/7 [==============================] - 1s 132ms/step - loss: 0.0381 - accuracy: 0.9857 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
            "Epoch 37/500\n",
            "7/7 [==============================] - 1s 126ms/step - loss: 0.0469 - accuracy: 0.9857 - val_loss: 0.0014 - val_accuracy: 1.0000 - lr: 1.2500e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute the classification metrics\n",
        "accuracy = accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "print('Accuracy:',accuracy.round(4))\n",
        "print('Precision:',precision.round(4))\n",
        "print('Recall:',recall.round(4))\n",
        "print('F1:',f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm.T)#, xticklabels=list(labels.values()), yticklabels=list(labels.values()))\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Xlmj6RaRRvk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "1e4da132-d059-4405-9508-cfd8085629ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.95\n",
            "Precision: 0.9545\n",
            "Recall: 0.95\n",
            "F1: 0.9499\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/gdrive/MyDrive/polimi/NAML/NAML_proj/models/hiphopvclassical/hiphopvclassical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO-HKBQc9I7r",
        "outputId": "fd667ff1-ad16-49ea-bd80-e7c76761b48f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /gdrive/MyDrive/polimi/NAML/NAML_proj/models/hiphopvclassical/hiphopvclassical/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /gdrive/MyDrive/polimi/NAML/NAML_proj/models/hiphopvclassical/hiphopvclassical/assets\n"
          ]
        }
      ]
    }
  ]
}