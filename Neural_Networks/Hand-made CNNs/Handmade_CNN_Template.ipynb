{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Handmade_CNN_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TEMPLATE CNN"
   ],
   "metadata": {
    "id": "gKB_Sss8Ql-3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwPJ54sBQlFO"
   },
   "outputs": [],
   "source": [
    "#importing google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#setting the working directory\n",
    "%cd /gdrive/MyDrive/polimi/NAML/NAML_proj/"
   ],
   "metadata": {
    "id": "iYHj7NhnQs_-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "seed = 42"
   ],
   "metadata": {
    "id": "INc4ttEUQuxN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = []\n",
    "genres = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n",
    "n_genres = 10\n",
    "\n",
    "for genre, genre_number in genres.items():\n",
    "    for filename in os.listdir(f'dataset_old/genres/{genre}'):\n",
    "        songname = f'dataset_old/genres/{genre}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=29.7)\n",
    "        ps = librosa.feature.melspectrogram(y=y, sr=sr, hop_length = 256, n_fft = 512)\n",
    "        ps = librosa.power_to_db(ps**2)\n",
    "        dataset.append( (ps, genre_number) )\n",
    "    print(str(genre+' done'))"
   ],
   "metadata": {
    "id": "oiT5mkVqQ_lt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "order = np.arange(start = 0, stop = 100, step = 1)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "training = []\n",
    "validation = []\n",
    "test = []\n",
    "\n",
    "for i in range(n_genres):\n",
    "  shuffle = np.random.permutation(order)\n",
    "  for k in range(70):\n",
    "    training.append(dataset[i*100 + shuffle[k]])\n",
    "  for l in range(20):\n",
    "    validation.append(dataset[i*100 + shuffle[l+70]])\n",
    "  for m in range(10):\n",
    "    test.append(dataset[i*100 + shuffle[m+90]])"
   ],
   "metadata": {
    "id": "OSUK8JPgRFVF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, Y_train = zip(*training)\n",
    "X_valid, Y_valid = zip(*validation)\n",
    "X_test, Y_test = zip(*test)\n",
    "\n",
    "X_train = np.array([x.reshape( (128, 2559, 1) ) for x in X_train])\n",
    "X_valid = np.array([x.reshape( (128, 2559, 1) ) for x in X_valid])\n",
    "X_test = np.array([x.reshape( (128, 2559, 1) ) for x in X_test])\n",
    "\n",
    "Y_train = np.array(tfk.utils.to_categorical(Y_train, n_genres))\n",
    "Y_valid = np.array(tfk.utils.to_categorical(Y_valid, n_genres))\n",
    "Y_test = np.array(tfk.utils.to_categorical(Y_test, n_genres))"
   ],
   "metadata": {
    "id": "WA29NV-URMuG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_model(input_shape, n_units):\n",
    "# Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    conv1 = tfkl.Conv2D(\n",
    "        filters=4,\n",
    "        kernel_size=(5, 5),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(input_layer)\n",
    "    conv1_2 = tfkl.Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(conv1)\n",
    "    pool1 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(conv1_2)\n",
    "\n",
    "    conv2 = tfkl.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(pool1)\n",
    "    conv2_2 = tfkl.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(conv2)\n",
    "    pool2 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(conv2_2)\n",
    "\n",
    "    conv3 = tfkl.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(pool2)\n",
    "    pool3 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(conv3)\n",
    "\n",
    "    conv4 = tfkl.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(pool3)\n",
    "    conv4_2 = tfkl.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(conv4)\n",
    "    pool4 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(conv4_2)\n",
    "\n",
    "    conv5 = tfkl.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(pool4)\n",
    "    pool5 = tfkl.MaxPooling2D(\n",
    "        pool_size = (2, 2)\n",
    "    )(conv5)\n",
    "\n",
    "    conv6 = tfkl.Conv2D(\n",
    "        filters=286,\n",
    "        kernel_size=(1,1),\n",
    "        strides = (1, 1),\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n",
    "    )(pool5)\n",
    "    \n",
    "\n",
    "    global_average = tfkl.GlobalAveragePooling2D(name = 'GAP')(conv6)\n",
    "    global_average = tfkl.Dropout(0.3, seed=seed)(global_average)\n",
    "    \n",
    "    classifier_layer = tfkl.Dense(units=64, name='Classifier', activation='relu')(global_average)\n",
    "    #flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n",
    "    #flattening_layer = tfkl.Dropout(0.2, seed=seed)(flattening_layer)\n",
    "    #classifier_layer = tfkl.Dense(units=64, name='Classifier', activation='relu')(flattening_layer)\n",
    "    \n",
    "    classifier_layer = tfkl.Dropout(0.3, seed=seed)(classifier_layer)\n",
    "    classifier_layer_2 = tfkl.Dense(units=32, name='Classifier_2', activation='relu')(classifier_layer)\n",
    "    classifier_layer_2 = tfkl.Dropout(0.3, seed=seed)(classifier_layer_2)\n",
    "    output_layer = tfkl.Dense(units=n_units, activation='softmax', name='Output')(classifier_layer_2)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "lR2QkBoSQ8lW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = (128, 2559, 1)\n",
    "\n",
    "model = build_model(input_shape, n_genres)\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "zIitkAlGQ-mM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20, restore_best_weights=True)\n",
    "adaptive_LR = tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-4)\n",
    "\n",
    "standard_history = model.fit(\n",
    "    x = X_train,\n",
    "    y = Y_train,\n",
    "    epochs = 500,\n",
    "    batch_size = 64,\n",
    "    validation_data= (X_valid, Y_valid),\n",
    "    callbacks = [early_stopping, adaptive_LR]\n",
    "    )"
   ],
   "metadata": {
    "id": "yFUsDY6BRWi1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm.T)#, xticklabels=list(labels.values()), yticklabels=list(labels.values()))\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "6Xlmj6RaRRvk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save('/gdrive/MyDrive/polimi/NAML/NAML_proj/models/***')"
   ],
   "metadata": {
    "id": "nCV2WgveT4Hb"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}